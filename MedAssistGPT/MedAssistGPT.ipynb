{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MedAssist.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/17YokGk0b0BEg6UFQppdnfeJM02eRUUhR\n",
    "\n",
    "MedAssist-GPT: Complete Medical LLM Pretraining Script\n",
    "=====================================================\n",
    "Modern architecture with RoPE, GQA, SwiGLU, RMSNorm\n",
    "Optimized for A100 GPU with Flash Attention\n",
    "Automatic checkpointing and HuggingFace uploads\n",
    "\n",
    "## Test\n",
    "\n",
    "## Imports\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import re\n",
    "import html\n",
    "import math\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & ML\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import wandb\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from huggingface_hub import login, create_repo, upload_folder, HfApi\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup, NavigableString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## CONFIGURATION\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"vocab_size\": 50281,\n",
    "    \"d_model\": 1024,\n",
    "    \"n_heads\": 16,\n",
    "    \"gqa_groups\": 4,\n",
    "    \"max_len\": 1024,\n",
    "    \"d_ff\": 2560,\n",
    "    \"eps\": 1e-5,\n",
    "    \"dropout_p\": 0.0,\n",
    "    \"blocks\": 24,\n",
    "}  # ~400M params - keep as is\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"batch_size\": 32,\n",
    "    \"max_length\": 1024,\n",
    "    \"stride\": 512,              # CHANGED: 50% overlap for better coverage\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 6e-4,       # CHANGED: Higher LR, cosine will bring it down\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.95,\n",
    "    \"eps\": 1e-8,\n",
    "    \"warmup_steps\": 2000,        # CHANGED: Longer warmup for stability\n",
    "    \"max_steps\": 100_000,        # CHANGED: ~13B tokens, plenty for this model\n",
    "    \"eval_freq\": 2000,\n",
    "    \"eval_iter\": 200,\n",
    "    \"save_freq\": 5000,\n",
    "    \"grad_clip\": 1.0,            # CHANGED: Tighter clipping for stability\n",
    "    \"num_workers\": 4,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "DATA_CONFIG = {\n",
    "    \"dataset_name\": \"bigbio/pubmed_qa\",  # CHANGED: Better quality QA dataset\n",
    "    \"text_column\": \"context\",\n",
    "    \"max_length\": 1024,\n",
    "    \"stride\": 512,               # Match above\n",
    "    \"train_split\": 0.98,         # CHANGED: More training data\n",
    "    \"max_train_samples\": 5_000_000,  # CHANGED: More docs if available\n",
    "    \"chunk_size\": 10000,\n",
    "    \"use_clean\": False,\n",
    "}\n",
    "\n",
    "WANDB_CONFIG = {\n",
    "    \"project\": \"MedAssist-GPT-Pretraining\",\n",
    "    \"entity\": \"kunjcr2-dreamable\",  # Your wandb username\n",
    "    \"name\": \"medassist-303M-test\",\n",
    "}\n",
    "\n",
    "HF_CONFIG = {\n",
    "    \"repo_id\": \"kunjcr2/MedAssist-GPT-401M\",  # Change this!\n",
    "    \"upload_checkpoints\": True,\n",
    "    \"upload_frequency\": 20000,  # Upload every N steps\n",
    "}\n",
    "\n",
    "INFERENCE_CONFIG = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"temperature\": 0.6,\n",
    "    \"prompt_text\": \"A patient was admitted with severe headache. Initial assessment revealed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089163a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## Architecture\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bf6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    \"\"\"Rotary Position Embeddings (RoPE)\"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        assert d_model % 2 == 0, \"d_model must be even for RoPE\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Position indices - tensor (0,1,2,...,max_len) of size (max_len, 1)\n",
    "        self.register_buffer('position_ids', torch.arange(max_len).unsqueeze(1))\n",
    "\n",
    "        # Frequency terms\n",
    "        self.register_buffer(\n",
    "            'div_term',\n",
    "            torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "            # e^(2i*(-log(10000))/d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, position_offset: int = 0) -> torch.Tensor:\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # Get positions with offset for KV cache\n",
    "        position_ids = self.position_ids[position_offset:position_offset + seq_len]  # (seq_len, 1)\n",
    "\n",
    "        # Calculate angles\n",
    "        angles = position_ids * self.div_term  # (seq_len, d_model/2)\n",
    "        cos_vals = torch.cos(angles)\n",
    "        sin_vals = torch.sin(angles)\n",
    "\n",
    "        # Reshape for rotation\n",
    "        x_pairs = x.view(batch_size, seq_len, d_model // 2, 2)  # (b, s, d//2, 2)\n",
    "        x_even = x_pairs[..., 0]  # (b, s, d//2)\n",
    "        x_odd = x_pairs[..., 1]  # (b, s, d//2)\n",
    "\n",
    "        # Apply rotation\n",
    "        rotated_even = x_even * cos_vals - x_odd * sin_vals\n",
    "        rotated_odd = x_even * sin_vals + x_odd * cos_vals\n",
    "\n",
    "        # Reconstruct\n",
    "        rotated_pairs = torch.stack([rotated_even, rotated_odd], dim=-1)  # (b, s, d//2, 2)\n",
    "        rotated_x = rotated_pairs.view(batch_size, seq_len, d_model)  # (b, s, d)\n",
    "\n",
    "        return rotated_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Grouped Query Attention (GQA) with RoPE\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        n_heads: int = 8,\n",
    "        gqa_groups: int = 2,\n",
    "        max_len: int = 1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        assert n_heads % gqa_groups == 0, \"n_heads must be divisible by gqa_groups\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.gqa_groups = gqa_groups\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.n_kv_heads = n_heads // gqa_groups\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Projections (bias-free)\n",
    "        self.q_proj = nn.Linear(d_model, n_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(n_heads * self.head_dim, d_model, bias=False)\n",
    "\n",
    "        # RoPE for Q and K\n",
    "        self.rope_q = RoPE(d_model=n_heads * self.head_dim, max_len=max_len)\n",
    "        self.rope_k = RoPE(d_model=self.n_kv_heads * self.head_dim, max_len=max_len)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Determine position offset from cache\n",
    "        if past_key_value is not None:\n",
    "            # past_key_value: (past_k, past_v) each of shape (B, H_kv, past_len, D)\n",
    "            position_offset = past_key_value[0].shape[2]\n",
    "        else:\n",
    "            position_offset = 0\n",
    "\n",
    "        # Project Q, K, V\n",
    "        q = self.q_proj(x)  # (B, T, H*D)\n",
    "        k = self.k_proj(x)  # (B, T, H_kv*D)\n",
    "        v = self.v_proj(x)  # (B, T, H_kv*D)\n",
    "\n",
    "        # Apply RoPE with position offset\n",
    "        q = self.rope_q(q, position_offset=position_offset)\n",
    "        k = self.rope_k(k, position_offset=position_offset)\n",
    "\n",
    "        # Reshape to heads\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, T, D)\n",
    "        k = k.view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)  # (B, H_kv, T, D)\n",
    "        v = v.view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)  # (B, H_kv, T, D)\n",
    "\n",
    "        # Concatenate with past KV if present\n",
    "        if past_key_value is not None:\n",
    "            past_k, past_v = past_key_value\n",
    "            k = torch.cat([past_k, k], dim=2)  # (B, H_kv, past_len + T, D)\n",
    "            v = torch.cat([past_v, v], dim=2)  # (B, H_kv, past_len + T, D)\n",
    "\n",
    "        # Store new cache if needed (before GQA expansion)\n",
    "        new_cache = (k, v) if use_cache else None\n",
    "\n",
    "        # Expand K and V for GQA\n",
    "        expand_factor = self.n_heads // self.n_kv_heads\n",
    "        k_expanded = k.repeat_interleave(expand_factor, dim=1)  # (B, H, total_len, D)\n",
    "        v_expanded = v.repeat_interleave(expand_factor, dim=1)  # (B, H, total_len, D)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        # When using cache, we only have new queries attending to all keys\n",
    "        # is_causal=True only works when q and k have same length\n",
    "        if past_key_value is not None:\n",
    "            # During generation: q has length 1 (or few), k/v have full length\n",
    "            # Cannot use is_causal=True, need explicit causal mask or no mask\n",
    "            # For autoregressive generation of single token, no mask needed\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k_expanded, v_expanded,\n",
    "                attn_mask=None,\n",
    "                dropout_p=0.0,\n",
    "                is_causal=False  # Single query attends to all past + current\n",
    "            )\n",
    "        else:\n",
    "            # Training or first inference step: standard causal attention\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k_expanded, v_expanded,\n",
    "                attn_mask=None,\n",
    "                dropout_p=0.0,\n",
    "                is_causal=True\n",
    "            )\n",
    "\n",
    "        # Merge heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.n_heads * self.head_dim)\n",
    "\n",
    "        # Output projection\n",
    "        out = self.o_proj(out)\n",
    "\n",
    "        if use_cache:\n",
    "            return out, new_cache\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU_MLP(nn.Module):\n",
    "    \"\"\"SwiGLU Feed-Forward Network\"\"\"\n",
    "    def __init__(self, d_model: int = 512, d_ff: int = 2048):\n",
    "        super().__init__()\n",
    "        # Fused up + gate projection\n",
    "        self.w1 = nn.Linear(d_model, 2 * d_ff, bias=False)\n",
    "        # Down projection\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        up, gate = self.w1(x).chunk(2, dim=-1) # breaks it into 2 parts - (b, s, d_ff)\n",
    "        x = up * F.silu(gate)  # SwiGLU activation - (b,s,d_ff) * (b,s,d_ff) = (b,s,d_ff)\n",
    "        x = self.w2(x)  # (b,s,d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with pre-norm and residual connections\"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.rms1 = nn.RMSNorm(config[\"d_model\"], eps=config[\"eps\"])\n",
    "        self.rms2 = nn.RMSNorm(config[\"d_model\"], eps=config[\"eps\"])\n",
    "\n",
    "        self.attn = GroupedQueryAttention(\n",
    "            d_model=config[\"d_model\"],\n",
    "            n_heads=config[\"n_heads\"],\n",
    "            gqa_groups=config[\"gqa_groups\"],\n",
    "            max_len=config[\"max_len\"]\n",
    "        )\n",
    "\n",
    "        self.mlp = SwiGLU_MLP(\n",
    "            d_model=config[\"d_model\"],\n",
    "            d_ff=config[\"d_ff\"]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"dropout_p\"])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        # Pre-norm attention with optional cache\n",
    "        if use_cache:\n",
    "            attn_out, present = self.attn(self.rms1(x), past_key_value=layer_past, use_cache=True)\n",
    "            x = x + self.dropout(attn_out)\n",
    "        else:\n",
    "            x = x + self.dropout(self.attn(self.rms1(x)))\n",
    "            present = None\n",
    "\n",
    "        # Pre-norm MLP\n",
    "        x = x + self.dropout(self.mlp(self.rms2(x)))\n",
    "\n",
    "        if use_cache:\n",
    "            return x, present\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b0199",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MedAssistGPT(nn.Module):\n",
    "    \"\"\"Main model class\"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Token embeddings\n",
    "        self.embed = nn.Embedding(config[\"vocab_size\"], config[\"d_model\"])\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config[\"blocks\"])\n",
    "        ])\n",
    "\n",
    "        # Final RMSNorm\n",
    "        self.final_rms = nn.RMSNorm(config[\"d_model\"], eps=config[\"eps\"])\n",
    "\n",
    "        # Language model head (weight-tied with embeddings)\n",
    "        self.lm_head = nn.Linear(config[\"d_model\"], config[\"vocab_size\"], bias=False)\n",
    "        self.lm_head.weight = self.embed.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: (batch, seq_len)\n",
    "            past_key_values: List of (k, v) tuples, one per layer. None for training.\n",
    "            use_cache: If True, return (logits, new_past_key_values). If False, return logits only.\n",
    "        \"\"\"\n",
    "        # input_ids: (batch, seq_len)\n",
    "        h = self.embed(input_ids)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Initialize presents list for caching\n",
    "        presents = [] if use_cache else None\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            layer_past = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if use_cache:\n",
    "                h, present = block(h, layer_past=layer_past, use_cache=True)\n",
    "                presents.append(present)\n",
    "            else:\n",
    "                h = block(h)\n",
    "\n",
    "        # Final normalization\n",
    "        h = self.final_rms(h)\n",
    "\n",
    "        # Language model head\n",
    "        logits = self.lm_head(h)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        if use_cache:\n",
    "            return logits, presents\n",
    "        return logits\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e420b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## Data Loading\n",
    "\n",
    "### clean()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67b30d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clean(xml_str: str) -> str:\n",
    "    \"\"\"\n",
    "    YOUR ORIGINAL XML CLEANING FUNCTION\n",
    "    Converts JATS/PMC XML to readable plain text\n",
    "    Removes tables, figures, citations, keeps narrative text\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(xml_str, \"lxml-xml\")\n",
    "\n",
    "    # 1) Remove whole non-narrative blocks\n",
    "    drop_whole = [\n",
    "        \"ref-list\", \"fig\", \"fig-group\", \"table-wrap\", \"table\", \"thead\", \"tbody\",\n",
    "        \"tr\", \"td\", \"th\", \"graphic\", \"media\", \"supplementary-material\", \"back\",\n",
    "        \"sec-meta\", \"table-wrap-foot\", \"caption\"\n",
    "    ]\n",
    "    for name in drop_whole:\n",
    "        for tag in soup.find_all(name):\n",
    "            tag.decompose()\n",
    "\n",
    "    # 2) Remove cross-references entirely (citations, table/fig pointers)\n",
    "    for tag in soup.find_all(\"xref\"):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 3) Preserve disp-quote as plain paragraphs\n",
    "    for dq in soup.find_all(\"disp-quote\"):\n",
    "        txt = dq.get_text(\" \", strip=True)\n",
    "        dq.replace_with(NavigableString((\"\\n\" + txt + \"\\n\") if txt else \"\"))\n",
    "\n",
    "    # 4) Turn <title> into clean section headers\n",
    "    for t in soup.find_all(\"title\"):\n",
    "        title_txt = t.get_text(\" \", strip=True)\n",
    "        t.replace_with(NavigableString(\"\\n\\n\" + title_txt + \"\\n\"))\n",
    "\n",
    "    # 5) Ensure paragraphs end cleanly; unwrap inline tags\n",
    "    inline_unwrap = [\n",
    "        \"italic\", \"bold\", \"underline\", \"sc\", \"em\", \"strong\", \"sup\", \"sub\",\n",
    "        \"styled-content\", \"inline-formula\", \"monospace\"\n",
    "    ]\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        for name in inline_unwrap:\n",
    "            for it in p.find_all(name):\n",
    "                it.unwrap()\n",
    "        p.insert_after(NavigableString(\"\\n\\n\"))\n",
    "        p.unwrap()\n",
    "\n",
    "    # 6) Unwrap remaining structural containers\n",
    "    for name in [\"sec\", \"body\", \"front\", \"article\", \"abstract\", \"boxed-text\", \"list\", \"list-item\"]:\n",
    "        for tag in soup.find_all(name):\n",
    "            tag.unwrap()\n",
    "\n",
    "    # 7) Extract text and clean up\n",
    "    text = soup.get_text()\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Post-processing cleanup\n",
    "    text = re.sub(r\"\\(\\s*(?:\\d+\\s*(?:[-–]\\s*\\d+)?\\s*(?:[,;]\\s*)?)+\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\(\\s*(?:[Ff]ig(?:ure)?\\.?\\s*\\d+|[Tt]able\\s*\\d+)\\s*\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\(\\s*\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\[\\s*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", text)\n",
    "    text = re.sub(r\"([,.;:!?])\\s*\\1+\", r\"\\1 \", text)\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94280a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### MemortMappedDataset class\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1996ab3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MemoryMappedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that uses memory-mapped files for ZERO RAM overhead!\n",
    "\n",
    "    HOW IT WORKS:\n",
    "    1. Data is stored on disk in binary format (.npy files)\n",
    "    2. When you access data[i], OS loads ONLY that piece into RAM\n",
    "    3. OS automatically evicts old data when RAM gets full\n",
    "    4. You get 100GB+ dataset working with 5GB RAM!\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_dir: Path):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "\n",
    "        # Load metadata (tiny file, ~1KB)\n",
    "        with open(self.cache_dir / \"metadata.pkl\", \"rb\") as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "\n",
    "        # Memory-map the input/target arrays\n",
    "        # mode='r' = read-only, mmap_mode='r' = memory-mapped read\n",
    "        # CRITICAL: np.load with mmap_mode DOES NOT load data into RAM!\n",
    "        # It just maps the file, OS loads pages on-demand\n",
    "        self.inputs_mmap = np.load(\n",
    "            self.cache_dir / \"inputs.npy\",\n",
    "            mmap_mode='r'  # ← THIS IS THE MAGIC! OS manages memory\n",
    "        )\n",
    "        self.targets_mmap = np.load(\n",
    "            self.cache_dir / \"targets.npy\",\n",
    "            mmap_mode='r'\n",
    "        )\n",
    "\n",
    "        print(f\"Memory-mapped dataset: {len(self.inputs_mmap):,} samples\")\n",
    "        print(f\"RAM overhead: ~0 MB (OS manages it)\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.inputs_mmap)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # OS loads ONLY this row into RAM (4KB per sample)\n",
    "        # When RAM is full, OS evicts least-recently-used data\n",
    "        input_ids = torch.from_numpy(self.inputs_mmap[idx].copy()).long()\n",
    "        target_ids = torch.from_numpy(self.targets_mmap[idx].copy()).long()\n",
    "\n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf78969",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### process_single_chunk()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996d0c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_single_chunk(\n",
    "    doc_batch: List[Dict],\n",
    "    tokenizer_name: str,  # Can't pickle tokenizer, so pass name\n",
    "    max_length: int,\n",
    "    stride: int,\n",
    "    use_clean: bool,\n",
    "    text_column: str,\n",
    "    chunk_id: int,\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray], int, int]:\n",
    "    \"\"\"\n",
    "    Worker function that processes one chunk\n",
    "\n",
    "    WHY SEPARATE FUNCTION:\n",
    "    - ProcessPoolExecutor spawns new Python processes\n",
    "    - Each process needs its own tokenizer instance\n",
    "    - Can't share tokenizer objects across processes (pickling issues)\n",
    "\n",
    "    WHAT IT DOES:\n",
    "    1. Creates its own tokenizer\n",
    "    2. Processes its batch of documents\n",
    "    3. Returns results to main process\n",
    "    \"\"\"\n",
    "    import tiktoken\n",
    "\n",
    "    # Each worker creates its own tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_name)\n",
    "    vocab_size = tokenizer.n_vocab\n",
    "\n",
    "    chunk_tokens = []\n",
    "    docs_processed = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Process each document in this chunk\n",
    "    for doc in doc_batch:\n",
    "        # Extract text\n",
    "        if text_column in doc:\n",
    "            text = doc[text_column]\n",
    "        elif 'text' in doc:\n",
    "            text = doc['text']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not text or len(text) < 100:\n",
    "            continue\n",
    "\n",
    "        # Apply clean function if enabled\n",
    "        if use_clean:\n",
    "            try:\n",
    "                text = clean(text)  # Your clean function\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if not text or len(text) < 100:\n",
    "            continue\n",
    "\n",
    "        # Tokenize\n",
    "        try:\n",
    "            tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "            tokens = [t for t in tokens if 0 <= t < vocab_size]\n",
    "\n",
    "            if len(tokens) < 10:\n",
    "                continue\n",
    "\n",
    "            chunk_tokens.extend(tokens)\n",
    "            total_tokens += len(tokens)\n",
    "            docs_processed += 1\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Create sliding windows from all tokens in this chunk\n",
    "    samples = create_sliding_windows(chunk_tokens, max_length, stride, vocab_size)\n",
    "\n",
    "    if samples:\n",
    "        inputs, targets = zip(*samples)\n",
    "        return list(inputs), list(targets), docs_processed, total_tokens\n",
    "    else:\n",
    "        return [], [], docs_processed, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bccc9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### process_dataset_in_chunks()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58055462",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_dataset_in_chunks(\n",
    "    dataset_name: str,\n",
    "    tokenizer,\n",
    "    cache_dir: Path,\n",
    "    max_length: int = 1024,\n",
    "    stride: int = 1024,\n",
    "    max_samples: int = None,\n",
    "    chunk_size: int = 5000,\n",
    "    use_clean: bool = True,\n",
    "    text_column: str = \"full_text\"\n",
    "):\n",
    "    \"\"\"\n",
    "    PARALLEL VERSION - Drop-in replacement for your original function\n",
    "\n",
    "    HOW PARALLELIZATION WORKS:\n",
    "    1. Load documents in batches (streaming)\n",
    "    2. Distribute batches to worker processes\n",
    "    3. Each worker: clean → tokenize → create windows\n",
    "    4. Main process: collect results and save\n",
    "\n",
    "    NUM WORKERS:\n",
    "    - Uses all CPU cores by default\n",
    "    - On 8-core: 8 chunks processed simultaneously\n",
    "    - On Colab: ~2 cores (still 2x speedup!)\n",
    "    \"\"\"\n",
    "\n",
    "    cache_dir = Path(cache_dir)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Determine number of workers\n",
    "    num_workers = min(mp.cpu_count(), 8)  # Max 8 workers (diminishing returns)\n",
    "\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"Chunk size: {chunk_size} documents\")\n",
    "    print(f\"Cleaning enabled: {use_clean}\")\n",
    "    print(f\"Parallel workers: {num_workers}\")\n",
    "\n",
    "    # Load dataset in streaming mode\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "    if max_samples:\n",
    "        dataset = dataset.take(max_samples)\n",
    "\n",
    "    # Accumulate results from all workers\n",
    "    all_inputs = []\n",
    "    all_targets = []\n",
    "    total_docs_processed = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    print(\"Loading documents into batches...\")\n",
    "\n",
    "    # Collect documents into batches\n",
    "    # WHY: Can't parallelize streaming iterator directly\n",
    "    # So we batch first, then parallelize batch processing\n",
    "    doc_batches = []\n",
    "    current_batch = []\n",
    "\n",
    "    for doc in tqdm(dataset, desc=\"Batching docs\"):\n",
    "        current_batch.append(doc)\n",
    "\n",
    "        if len(current_batch) >= chunk_size:\n",
    "            doc_batches.append(current_batch)\n",
    "            current_batch = []\n",
    "\n",
    "    # Don't forget the last batch\n",
    "    if current_batch:\n",
    "        doc_batches.append(current_batch)\n",
    "\n",
    "    print(f\"Created {len(doc_batches)} batches of ~{chunk_size} documents each\")\n",
    "    print(f\"Processing with {num_workers} parallel workers...\")\n",
    "\n",
    "    # PARALLEL PROCESSING STARTS HERE!\n",
    "    # ProcessPoolExecutor creates worker processes\n",
    "    # Each worker gets its own batch to process\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "\n",
    "        # Submit all batches to workers\n",
    "        # partial() pre-fills the arguments that are same for all workers\n",
    "        worker_fn = partial(\n",
    "            process_single_chunk,\n",
    "            tokenizer_name=\"p50k_base\",  # Pass tokenizer name, not object\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            use_clean=use_clean,\n",
    "            text_column=text_column,\n",
    "        )\n",
    "\n",
    "        # Submit all jobs\n",
    "        futures = []\n",
    "        for chunk_id, doc_batch in enumerate(doc_batches):\n",
    "            future = executor.submit(worker_fn, doc_batch, chunk_id=chunk_id)\n",
    "            futures.append(future)\n",
    "\n",
    "        # Collect results as they complete\n",
    "        # as_completed() returns futures as soon as they finish (not in order)\n",
    "        # This gives us progress updates in real-time!\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing chunks\"):\n",
    "            try:\n",
    "                # Get results from this worker\n",
    "                inputs, targets, docs_proc, tokens_proc = future.result()\n",
    "\n",
    "                # Accumulate\n",
    "                all_inputs.extend(inputs)\n",
    "                all_targets.extend(targets)\n",
    "                total_docs_processed += docs_proc\n",
    "                total_tokens += tokens_proc\n",
    "\n",
    "                # Show progress\n",
    "                if len(inputs) > 0:\n",
    "                    print(f\"Chunk done: {len(inputs)} samples, {docs_proc} docs, {tokens_proc:,} tokens\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Worker failed: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"\\nParallel processing complete!\")\n",
    "    print(f\"   Documents processed: {total_docs_processed:,}\")\n",
    "    print(f\"   Total tokens: {total_tokens:,}\")\n",
    "    print(f\"   Training samples: {len(all_inputs):,}\")\n",
    "\n",
    "    # Save to disk (same as before)\n",
    "    print(\"Saving to disk as memory-mapped arrays...\")\n",
    "\n",
    "    inputs_array = np.array(all_inputs, dtype=np.int32)\n",
    "    targets_array = np.array(all_targets, dtype=np.int32)\n",
    "\n",
    "    np.save(cache_dir / \"inputs.npy\", inputs_array)\n",
    "    np.save(cache_dir / \"targets.npy\", targets_array)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"num_samples\": len(all_inputs),\n",
    "        \"max_length\": max_length,\n",
    "        \"stride\": stride,\n",
    "        \"vocab_size\": tokenizer.n_vocab,\n",
    "        \"docs_processed\": total_docs_processed,\n",
    "        \"total_tokens\": total_tokens,\n",
    "    }\n",
    "\n",
    "    with open(cache_dir / \"metadata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(f\"Saved to {cache_dir}\")\n",
    "\n",
    "    # Clear RAM\n",
    "    del all_inputs, all_targets, inputs_array, targets_array\n",
    "    gc.collect()\n",
    "\n",
    "    return cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ffc3c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### create_sliding_windows()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a486379",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_sliding_windows(\n",
    "    tokens: List[int],\n",
    "    max_length: int,\n",
    "    stride: int,\n",
    "    vocab_size: int\n",
    ") -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Create sliding window samples from token list\n",
    "\n",
    "    EXAMPLE:\n",
    "    tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    max_length = 4, stride = 2\n",
    "\n",
    "    Samples:\n",
    "    input:  [1, 2, 3, 4]  target: [2, 3, 4, 5]\n",
    "    input:  [3, 4, 5, 6]  target: [4, 5, 6, 7]\n",
    "    input:  [5, 6, 7, 8]  target: [6, 7, 8, 9]\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    tokens = np.array(tokens, dtype=np.int32)\n",
    "\n",
    "    # Verify tokens are in valid range\n",
    "    if len(tokens) == 0:\n",
    "        return samples\n",
    "\n",
    "    # Clip to valid range (safety)\n",
    "    tokens = np.clip(tokens, 0, vocab_size - 1)\n",
    "\n",
    "    # Create sliding windows\n",
    "    for i in range(0, len(tokens) - max_length, stride):\n",
    "        input_ids = tokens[i:i+max_length]\n",
    "        target_ids = tokens[i+1:i+max_length+1]\n",
    "\n",
    "        # Ensure both are exactly max_length\n",
    "        if len(input_ids) == max_length and len(target_ids) == max_length:\n",
    "            samples.append((input_ids, target_ids))\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a32e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### prepare_mdeical_data()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329e93d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_medical_data(\n",
    "    config: Dict[str, Any],\n",
    "    tokenizer\n",
    ") -> Tuple[Path, Path]:\n",
    "    \"\"\"\n",
    "    Main function: Prepares training and validation data efficiently\n",
    "\n",
    "    WHAT THIS DOES:\n",
    "    1. Checks if cache exists (skip if already processed)\n",
    "    2. Splits dataset into train/val\n",
    "    3. Processes each split in chunks\n",
    "    4. Saves as memory-mapped files\n",
    "    5. Returns paths to cached data\n",
    "\n",
    "    FIRST RUN: ~30 minutes (processes and caches)\n",
    "    SUBSEQUENT RUNS: ~1 second (loads from cache)\n",
    "    \"\"\"\n",
    "\n",
    "    train_cache = Path(\"./data_cache/train\")\n",
    "    val_cache = Path(\"./data_cache/val\")\n",
    "\n",
    "    # Check if already cached\n",
    "    train_exists = (train_cache / \"metadata.pkl\").exists()\n",
    "    val_exists = (val_cache / \"metadata.pkl\").exists()\n",
    "\n",
    "    if train_exists and val_exists:\n",
    "        print(\"Found cached data! Skipping processing.\")\n",
    "        print(f\"   Train cache: {train_cache}\")\n",
    "        print(f\"   Val cache: {val_cache}\")\n",
    "        return train_cache, val_cache\n",
    "\n",
    "    # Calculate split sizes\n",
    "    total_samples = config.get(\"max_train_samples\", 1_000_000)\n",
    "    train_size = int(total_samples * config.get(\"train_split\", 0.95))\n",
    "    val_size = total_samples - train_size\n",
    "\n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"   Training: {train_size:,} documents\")\n",
    "    print(f\"   Validation: {val_size:,} documents\")\n",
    "\n",
    "    # Process training data\n",
    "    if not train_exists:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PROCESSING TRAINING DATA\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Create temporary streaming dataset for train split\n",
    "        dataset = load_dataset(\n",
    "            config[\"dataset_name\"],\n",
    "            split=\"train\",\n",
    "            streaming=True\n",
    "        )\n",
    "        train_dataset = dataset.take(train_size)\n",
    "\n",
    "        # Process and cache\n",
    "        # We create a temporary generator to process\n",
    "        def train_generator():\n",
    "            for item in train_dataset:\n",
    "                yield item\n",
    "\n",
    "        # Process using your dataset directly\n",
    "        process_dataset_in_chunks(\n",
    "            dataset_name=config[\"dataset_name\"],\n",
    "            tokenizer=tokenizer,\n",
    "            cache_dir=train_cache,\n",
    "            max_length=config.get(\"max_length\", 1024),\n",
    "            stride=config.get(\"stride\", 1024),\n",
    "            max_samples=train_size,\n",
    "            chunk_size=config.get(\"chunk_size\", 5000),\n",
    "            use_clean=config.get(\"use_clean\", True),\n",
    "            text_column=config.get(\"text_column\", \"full_text\")\n",
    "        )\n",
    "\n",
    "    # Process validation data\n",
    "    if not val_exists:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PROCESSING VALIDATION DATA\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Create dataset that skips training samples\n",
    "        dataset = load_dataset(\n",
    "            config[\"dataset_name\"],\n",
    "            split=\"train\",\n",
    "            streaming=True\n",
    "        )\n",
    "        val_dataset = dataset.skip(train_size).take(val_size)\n",
    "\n",
    "        # Process and cache\n",
    "        # For validation, we need to handle the skip\n",
    "        # Easier to just process with offset\n",
    "        temp_config = config.copy()\n",
    "        temp_config[\"dataset_offset\"] = train_size\n",
    "\n",
    "        process_dataset_in_chunks(\n",
    "            dataset_name=config[\"dataset_name\"],\n",
    "            tokenizer=tokenizer,\n",
    "            cache_dir=val_cache,\n",
    "            max_length=config.get(\"max_length\", 1024),\n",
    "            stride=config.get(\"stride\", 1024),\n",
    "            max_samples=val_size,\n",
    "            chunk_size=config.get(\"chunk_size\", 5000),\n",
    "            use_clean=config.get(\"use_clean\", True),\n",
    "            text_column=config.get(\"text_column\", \"full_text\")\n",
    "        )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA PREPARATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return train_cache, val_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b151c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### create_dataloader()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467fe1cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_dataloader(\n",
    "    cache_dir: Path,\n",
    "    batch_size: int,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 2,  # Lower for memory-mapped files\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create DataLoader from cached data\n",
    "\n",
    "    NUM_WORKERS = 2:\n",
    "    - Memory-mapped files don't benefit from many workers\n",
    "    - OS handles the parallel I/O better than Python\n",
    "    - 2 workers is sweet spot for prefetching\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = MemoryMappedDataset(cache_dir)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        drop_last=True,\n",
    "        persistent_workers=True,  # Keep workers alive between epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035715b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## Training Utils\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281094d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(\n",
    "    input_batch: torch.Tensor,\n",
    "    target_batch: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Calculate loss for a single batch\"\"\"\n",
    "    input_batch = input_batch.to(device, non_blocking=True)\n",
    "    target_batch = target_batch.to(device, non_blocking=True)\n",
    "\n",
    "    with autocast(\"cuda\", torch.bfloat16):\n",
    "        logits = model(input_batch)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.flatten(0, 1),\n",
    "            target_batch.flatten()\n",
    "        )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_batches: int = 100\n",
    ") -> float:\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7923e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Any,\n",
    "    step: int,\n",
    "    loss: float,\n",
    "    save_dir: Path,\n",
    "    config: Dict[str, Any]\n",
    "):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': config,\n",
    "    }\n",
    "\n",
    "    save_path = save_dir / f\"checkpoint_step_{step}.pt\"\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved: {save_path}\")\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379489b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def upload_to_huggingface(\n",
    "    model: nn.Module,\n",
    "    save_dir: Path,\n",
    "    repo_id: str,\n",
    "    config: Dict[str, Any],\n",
    "    step: int\n",
    "):\n",
    "    \"\"\"Upload model to HuggingFace Hub\"\"\"\n",
    "    try:\n",
    "        # Save model weights\n",
    "        torch.save(model.state_dict(), save_dir / \"pytorch_model.bin\")\n",
    "\n",
    "        # Save config\n",
    "        with open(save_dir / \"config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "\n",
    "        # Upload to HF\n",
    "        api = HfApi()\n",
    "        api.upload_folder(\n",
    "            folder_path=str(save_dir),\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"Training checkpoint at step {step}\"\n",
    "        )\n",
    "\n",
    "        print(f\"Uploaded to HuggingFace: {repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to HuggingFace: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630deef6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## Training loop\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc20544",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Any,\n",
    "    device: torch.device,\n",
    "    config: Dict[str, Any],\n",
    "    save_dir: Path,\n",
    "    hf_repo_id: str = None\n",
    "):\n",
    "    \"\"\"Main training loop with all optimizations\"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STARTING MEDICAL LLM PRETRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {model.count_parameters():,} parameters\")\n",
    "    print(f\"Training batches: {len(train_loader):,}\")\n",
    "    print(f\"Max steps: {config['max_steps']:,}\")\n",
    "    print(f\"Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    tokens_seen = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    grad_accum = config[\"gradient_accumulation_steps\"]\n",
    "\n",
    "    try:\n",
    "        for epoch in range(100):  # Virtually unlimited epochs\n",
    "            epoch_loss = 0.0\n",
    "            epoch_steps = 0\n",
    "\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "            for batch_idx, (input_batch, target_batch) in enumerate(progress_bar):\n",
    "                # Forward pass\n",
    "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / grad_accum\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate\n",
    "                if (batch_idx + 1) % grad_accum == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    # Clip gradients\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(),\n",
    "                        max_norm=config[\"grad_clip\"]\n",
    "                    )\n",
    "\n",
    "                    # Optimizer step\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Update counters\n",
    "                    global_step += 1\n",
    "                    tokens_seen += input_batch.numel() * grad_accum\n",
    "\n",
    "                    # Log training loss\n",
    "                    # train_losses.append(loss.item() * grad_accum)\n",
    "                    epoch_loss += loss.item() * grad_accum\n",
    "                    epoch_steps += 1\n",
    "\n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f\"{loss.item() * grad_accum:.4f}\",\n",
    "                        'lr': f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "                        'step': global_step\n",
    "                    })\n",
    "\n",
    "                    # Evaluation\n",
    "                    if global_step % config[\"eval_freq\"] == 0:\n",
    "                        val_loss = evaluate_model(\n",
    "                            model, val_loader, device, config[\"eval_iter\"]\n",
    "                        )\n",
    "                        # val_losses.append(val_loss)\n",
    "\n",
    "                        # Log to wandb\n",
    "                        wandb.log({\n",
    "                            \"val_loss\": val_loss,\n",
    "                            \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "                            \"step\": global_step,\n",
    "                        })\n",
    "\n",
    "                        # Check for improvement\n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            print(f\"\\nNew best validation loss: {val_loss:.4f}\")\n",
    "\n",
    "                    # Save checkpoint\n",
    "                    if global_step % config[\"save_freq\"] == 0:\n",
    "                        save_checkpoint(\n",
    "                            model, optimizer, scheduler,\n",
    "                            global_step, train_losses[-1],\n",
    "                            save_dir, MODEL_CONFIG\n",
    "                        )\n",
    "\n",
    "                        # Upload to HuggingFace\n",
    "                        if hf_repo_id and config.get(\"upload_checkpoints\", False):\n",
    "                            if global_step % config.get(\"upload_frequency\", 1000) == 0:\n",
    "                                upload_to_huggingface(\n",
    "                                    model, save_dir / \"hf_upload\",\n",
    "                                    hf_repo_id, MODEL_CONFIG, global_step\n",
    "                                )\n",
    "\n",
    "                    # Check if max steps reached\n",
    "                    if global_step >= config[\"max_steps\"]:\n",
    "                        print(f\"\\nReached max steps ({config['max_steps']})\")\n",
    "                        raise StopIteration\n",
    "\n",
    "                    wandb.log({\n",
    "                        \"train_loss\": loss.item() * grad_accum,\n",
    "                        \"tokens_seen\": tokens_seen\n",
    "                    })\n",
    "\n",
    "            # End of epoch summary\n",
    "            avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else float('inf')\n",
    "            print(f\"\\nEpoch {epoch+1} complete - Avg loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    except (KeyboardInterrupt, StopIteration):\n",
    "        print(\"\\nTraining stopped\")\n",
    "\n",
    "    # Final checkpoint\n",
    "    print(\"\\nSaving final checkpoint...\")\n",
    "    save_checkpoint(\n",
    "        model, optimizer, scheduler,\n",
    "        global_step, train_losses[-1] if train_losses else 0,\n",
    "        save_dir, MODEL_CONFIG\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\" Total steps: {global_step:,}\")\n",
    "    print(f\" Total tokens: {tokens_seen:,}\")\n",
    "    print(f\" Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3444a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## main.py\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f727a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(TRAINING_CONFIG[\"seed\"])\n",
    "    np.random.seed(TRAINING_CONFIG[\"seed\"])\n",
    "\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create save directory\n",
    "    save_dir = Path(\"./checkpoints\")\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
    "\n",
    "    # Load and prepare data\n",
    "    train_tokens, val_tokens = prepare_medical_data(DATA_CONFIG, tokenizer)\n",
    "\n",
    "    # Create dataloaders\n",
    "    print(\"Creating dataloaders...\")\n",
    "    train_loader = create_dataloader(\n",
    "        Path(\"/content/data_cache/train/\"),\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        # max_length=TRAINING_CONFIG[\"max_length\"],\n",
    "        # stride=TRAINING_CONFIG[\"stride\"],\n",
    "        shuffle=True,\n",
    "        num_workers=TRAINING_CONFIG[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    val_loader = create_dataloader(\n",
    "        Path(\"/content/data_cache/val/\"),\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        # max_length=TRAINING_CONFIG[\"max_length\"],\n",
    "        # stride=TRAINING_CONFIG[\"stride\"],\n",
    "        shuffle=False,\n",
    "        num_workers=TRAINING_CONFIG[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    model = MedAssistGPT(MODEL_CONFIG)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Compile model (PyTorch 2.0+)\n",
    "    if hasattr(torch, 'compile'):\n",
    "        print(\"Compiling model...\")\n",
    "        model = torch.compile(model, mode=\"default\", fullgraph=False, dynamic=True)\n",
    "\n",
    "    print(f\"Model has {model.count_parameters():,} parameters\")\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=TRAINING_CONFIG[\"learning_rate\"],\n",
    "        weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
    "        betas=(TRAINING_CONFIG[\"beta1\"], TRAINING_CONFIG[\"beta2\"]),\n",
    "        eps=TRAINING_CONFIG[\"eps\"]\n",
    "    )\n",
    "\n",
    "    # Initialize scheduler\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=TRAINING_CONFIG[\"learning_rate\"],\n",
    "        total_steps=TRAINING_CONFIG[\"max_steps\"],\n",
    "        pct_start=TRAINING_CONFIG[\"warmup_steps\"] / TRAINING_CONFIG[\"max_steps\"],\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=10,\n",
    "        final_div_factor=100\n",
    "    )\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=WANDB_CONFIG[\"project\"],\n",
    "        entity=WANDB_CONFIG[\"entity\"],\n",
    "        name=WANDB_CONFIG[\"name\"],\n",
    "        config={**MODEL_CONFIG, **TRAINING_CONFIG, **DATA_CONFIG}\n",
    "    )\n",
    "\n",
    "    # Login to HuggingFace (if uploading)\n",
    "    if HF_CONFIG.get(\"upload_checkpoints\", False):\n",
    "        try:\n",
    "            login()\n",
    "            create_repo(HF_CONFIG[\"repo_id\"], repo_type=\"model\", exist_ok=True)\n",
    "            print(f\"HuggingFace repo ready: {HF_CONFIG['repo_id']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"HuggingFace setup failed: {e}\")\n",
    "            HF_CONFIG[\"upload_checkpoints\"] = False\n",
    "\n",
    "\n",
    "    # Train!\n",
    "    train(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        config=TRAINING_CONFIG,\n",
    "        save_dir=save_dir,\n",
    "        hf_repo_id=HF_CONFIG[\"repo_id\"] if HF_CONFIG.get(\"upload_checkpoints\") else None\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"\\nAll done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb318dcd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"## Inference\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7d0f8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, inference_config: Dict[str, Any], device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Generate text using KV cache for efficient autoregressive generation.\n",
    "\n",
    "    First step: Process the full prompt, cache K/V.\n",
    "    Subsequent steps: Only feed the new token, reuse cached K/V.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    text = inference_config[\"prompt_text\"]\n",
    "    max_new_tokens = inference_config.get(\"max_new_tokens\", 50)\n",
    "    temperature = inference_config.get(\"temperature\", 0.8)\n",
    "\n",
    "    # Encode the input text\n",
    "    encoded_input = tokenizer.encode(text)\n",
    "    input_ids = torch.tensor(encoded_input, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_tokens = []\n",
    "    past_key_values = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            # First step: process full prompt\n",
    "            # Subsequent steps: only feed the last generated token\n",
    "            if past_key_values is None:\n",
    "                current_input = input_ids\n",
    "            else:\n",
    "                current_input = input_ids[:, -1:]  # Only the new token\n",
    "\n",
    "            # Forward pass with caching\n",
    "            logits, past_key_values = model(current_input, past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "            # Get logits for the last position\n",
    "            next_token_logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "\n",
    "            # Apply temperature sampling\n",
    "            if temperature == 0.0:\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n",
    "            else:\n",
    "                probs = torch.softmax(next_token_logits / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append the new token\n",
    "            generated_tokens.append(next_token.item())\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "            # Stop if end-of-text token is generated\n",
    "            if next_token.item() == tokenizer.eot_token:\n",
    "                break\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    decoded_output = tokenizer.decode(generated_tokens)\n",
    "    return text + decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8935c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path: Path, model_config: Dict[str, Any], device: torch.device) -> nn.Module:\n",
    "    \"\"\"Loads a MedAssistGPT model from a checkpoint file.\"\"\"\n",
    "    print(f\"Loading model from {checkpoint_path}...\")\n",
    "    model = MedAssistGPT(model_config)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Fix: Remove '_orig_mod.' prefix from state_dict keys if present\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('_orig_mod.'):\n",
    "            new_state_dict[k[len('_orig_mod.'):]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0669843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# First, set up device and tokenizer as they are needed for both loading and inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"p50k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your saved checkpoint\n",
    "# Replace 'checkpoints/checkpoint_step_15000.pt' with the actual path if different\n",
    "checkpoint_file_path = Path(\"checkpoints/checkpoint_step_15000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = load_model_from_checkpoint(checkpoint_file_path, MODEL_CONFIG, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afaac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use the loaded_model with the generate_text function\n",
    "print(\"\\n--- Generating text with loaded model ---\")\n",
    "generated_output = generate_text(loaded_model, tokenizer, INFERENCE_CONFIG, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6dd7c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(generated_output)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
