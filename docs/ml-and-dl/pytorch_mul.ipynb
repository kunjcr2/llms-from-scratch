{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab60a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ecb7c",
   "metadata": {},
   "source": [
    "1. `*` (elementwise multiply)\n",
    "\n",
    "* What it does: multiplies tensors elementwise (same shape after broadcasting). Supports broadcasting and dtype promotion; equivalent to `torch.mul(t1, t2)`. ([PyTorch Docs][1])\n",
    "* Important args/notes: no explicit args (uses Tensor `__mul__`); broadcasting rules apply; result participates in autograd like any tensor op.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982b732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.,2.,3.])\n",
    "b = torch.tensor([10.,20.,30.])\n",
    "c = a * b   # tensor([10., 40., 90.])\n",
    "# broadcasting:\n",
    "x = torch.randn(2,3); s = torch.tensor([2.,3.,4.])\n",
    "y = x * s   # s broadcast across dim0\n",
    "\n",
    "w = torch.tensor([2,3, 4])\n",
    "z = w * s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd8da4",
   "metadata": {},
   "source": [
    "2. `torch.mul(input, other, *, out=None)` / `Tensor.mul` / `Tensor.mul_`\n",
    "\n",
    "* What it does: elementwise multiply (same semantics as `*`). `torch.mul` is the functional form; `Tensor.mul_` is the **in-place** variant that overwrites the tensor. `torch.mul` supports an `out=` keyword to write result into a provided tensor. ([PyTorch Docs][1])\n",
    "* Important args/notes:\n",
    "\n",
    "  * `other` can be scalar or tensor (broadcasting applies).\n",
    "  * `out` (optional): preallocated tensor to receive the result (avoids allocation).\n",
    "  * In-place `mul_()` modifies the receiver (breaks views/grad expectations if used carelessly).\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460a989e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.empty(3)\n",
    "torch.mul(a, b, out=z)   # writes into z\n",
    "a.mul_(2)                # a modified in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca6136",
   "metadata": {},
   "source": [
    "3. `@` (Python matrix-multiply operator → routes to `matmul`) / `torch.matmul(input, other, *, out=None)`\n",
    "\n",
    "* What it does: linear-algebra product with behavior depending on rank:\n",
    "\n",
    "  * 1D @ 1D → dot (scalar).\n",
    "  * 2D @ 2D → matrix×matrix.\n",
    "  * ND @ ND (N≥3) → batched matmul with broadcasting of batch dims. Note: the 1D dot case may not support `out=` in some versions. ([PyTorch Docs][2])\n",
    "* Important args/notes:\n",
    "\n",
    "  * `out` (optional) for most cases (check docs: 1D·1D may not accept `out`).\n",
    "  * Broadcasting rules for batch dims; shapes must be compatible for matrix multiplication on the last two dims.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b87847ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D tensor\n",
    "D = torch.tensor([3., 4.])\n",
    "\n",
    "# 2D matmul\n",
    "A = torch.randn(2,3); B = torch.randn(3,4)\n",
    "C = A @ B   # (2,4)\n",
    "\n",
    "# batched matmul\n",
    "X = torch.randn(5,2,3); W = torch.randn(5,3,4)\n",
    "Y = torch.matmul(X, W)   # (5,2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41287ff5",
   "metadata": {},
   "source": [
    "4. `torch.mm(input, mat2, *, out=None, out_dtype=None)` (strict 2D matrix multiply)\n",
    "\n",
    "* What it does: matrix multiplication **only** for 2-D tensors (no batching, no broadcasting across batch dims). Use for plain 2D matrix×matrix when you want the strict 2D contract. ([PyTorch Docs][3])\n",
    "* Important args/notes:\n",
    "\n",
    "  * Both inputs must be 2D.\n",
    "  * `out` optional. Some GPU backends expose `out_dtype` controls for precision on CUDA/ROCm.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c3e45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2,3); B = torch.randn(3,4)\n",
    "R = torch.mm(A, B)   # (2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cfeef",
   "metadata": {},
   "source": [
    "5. `torch.bmm(input, mat2, *, out=None, out_dtype=None)` (batched matrix multiply)\n",
    "\n",
    "* What it does: batched 2-D matmul for **3-D** tensors: `(B, N, M) @ (B, M, P) -> (B, N, P)`. No broadcasting over the batch dimension — batch sizes must match. ([PyTorch Docs][4])\n",
    "* Important args/notes:\n",
    "\n",
    "  * Inputs must be 3D and share the same batch size.\n",
    "  * `out` optional; `out_dtype` available on some devices.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59c52c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(10,2,3); B = torch.randn(10,3,4)\n",
    "C = torch.bmm(A, B)   # (10,2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087bbaf",
   "metadata": {},
   "source": [
    "6. `torch.einsum(equation, *operands, /, *, out=None)`\n",
    "\n",
    "* What it does: general Einstein summation—specify index labels to compute arbitrary contractions (matmul, batched matmul, trace, outer, inner, transpositions, etc.) by an equation string like `\"ij,jk->ik\"`. Extremely flexible and expressive. ([PyTorch Docs][5])\n",
    "* Important args/notes:\n",
    "\n",
    "  * `equation` is a string describing indices; operands must match the labels.\n",
    "  * `out` optional.\n",
    "  * Can express operations that would otherwise require combinations of permute/reshape/matmul; but can be slower for some simple ops—benchmark if performance-critical.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc684c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matmul\n",
    "C = torch.einsum(\"ij,jk->ik\", A, B)\n",
    "\n",
    "# batched matmul\n",
    "Y = torch.einsum(\"bij,bjk->bik\", X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6531233",
   "metadata": {},
   "source": [
    "7. `torch.dot(input, tensor, *, out=None)`\n",
    "\n",
    "* What it does: dot product of **two 1-D** tensors only (strict 1D contract). Unlike NumPy, `torch.dot` intentionally requires both inputs be 1-D and same length. ([PyTorch Docs][6])\n",
    "* Important args/notes:\n",
    "\n",
    "  * Inputs must be 1D and same size.\n",
    "  * `out` optional. For batched or higher-dim reductions, use `torch.inner`, `torch.matmul`, or `torch.linalg.vecdot`.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor([1.,2.,3.]); v = torch.tensor([10.,20.,30.])\n",
    "s = torch.dot(u, v)   # scalar 140."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9649156",
   "metadata": {},
   "source": [
    "8. `torch.inner(input, other, *, out=None)`\n",
    "\n",
    "* What it does: computes dot over the **last dimension**: for 1-D tensors it equals `dot`; for higher dims it sums products over the last axis of each tensor and broadcasts over leading axes. Useful for batched inner products. ([PyTorch Docs][7])\n",
    "* Important args/notes:\n",
    "\n",
    "  * If either input is scalar, behaves like elementwise multiply.\n",
    "  * Supports broadcasting on leading dims; result shape equals broadcasted leading dims.\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c85395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D\n",
    "torch.inner(u, v)   # scalar\n",
    "\n",
    "# batched\n",
    "x = torch.randn(4,3); y = torch.randn(4,3)\n",
    "r = torch.inner(x, y)   # (4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1d188",
   "metadata": {},
   "source": [
    "9. `torch.vdot(input, other, *, out=None)`\n",
    "\n",
    "* What it does: dot product that **conjugates the first argument** (for complex tensors) and then sums; flattens inputs to 1D first. For real tensors behaves like `dot`. Use when working with complex vectors and you need conjugation semantics. ([PyTorch Docs][8])\n",
    "* Important args/notes:\n",
    "\n",
    "  * Inputs are flattened; both should be 1-D-equivalent length.\n",
    "  * Conjugates `input` if complex (`conj(input) * other` summed).\n",
    "  * For batched vector-dot use `torch.linalg.vecdot` (different API).\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79775e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1+2j, 3+4j])\n",
    "b = torch.tensor([10+0j, 20+0j])\n",
    "s = torch.vdot(a, b)   # uses conj(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb09e87",
   "metadata": {},
   "source": [
    "10. `torch.outer(input, other)` and `torch.kron` (optional helpers often used) — brief mention\n",
    "\n",
    "* `torch.outer` computes the outer product of 1-D vectors → matrix; `torch.kron` computes the Kronecker product. Use these when you need expansions rather than contractions. (Docs: general torch reference.) ([PyTorch Docs][9])\n",
    "* Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = torch.outer(u, v)   # shape (3,3)\n",
    "k = torch.kron(torch.tensor([[1,2]]), torch.tensor([[3,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e4416",
   "metadata": {},
   "source": [
    "— end.\n",
    "\n",
    "[1]: https://docs.pytorch.org/docs/stable/generated/torch.mul.html?utm_source=chatgpt.com \"torch.mul — PyTorch 2.10 documentation\"\n",
    "[2]: https://docs.pytorch.org/docs/stable/generated/torch.matmul.html?utm_source=chatgpt.com \"torch.matmul — PyTorch 2.10 documentation\"\n",
    "[3]: https://docs.pytorch.org/docs/stable/generated/torch.mm.html?utm_source=chatgpt.com \"torch.mm — PyTorch 2.10 documentation\"\n",
    "[4]: https://docs.pytorch.org/docs/stable/generated/torch.bmm.html?utm_source=chatgpt.com \"torch.bmm — PyTorch 2.10 documentation\"\n",
    "[5]: https://docs.pytorch.org/docs/stable/generated/torch.einsum.html?utm_source=chatgpt.com \"torch.einsum — PyTorch 2.10 documentation\"\n",
    "[6]: https://docs.pytorch.org/docs/stable/generated/torch.dot.html?utm_source=chatgpt.com \"torch.dot — PyTorch 2.10 documentation\"\n",
    "[7]: https://docs.pytorch.org/docs/stable/generated/torch.inner.html?utm_source=chatgpt.com \"torch.inner — PyTorch 2.10 documentation\"\n",
    "[8]: https://docs.pytorch.org/docs/stable/generated/torch.vdot.html?utm_source=chatgpt.com \"torch.vdot — PyTorch 2.10 documentation\"\n",
    "[9]: https://docs.pytorch.org/docs/stable/torch.html?utm_source=chatgpt.com \"torch — PyTorch 2.9 documentation\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
