# -*- coding: utf-8 -*-
"""Audio Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1piFkaUpVdtICdJnoDtpZXpLopiz3gXes

# Audio Classification on Major and Minor
"""

# !pip install -q opendatasets
import opendatasets as od
od.download("https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification")

import torch
from torch import nn
from torch.optim import Adam
import librosa
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import LabelEncoder # Converts classes to 0-(n-1) categories
import matplotlib.pyplot as plt
from PIL import Image
import pandas as pd
import numpy as np
import time
import os

from skimage.transform import resize

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f'Using {device}')

audio_files = []
audio_labels = []

for i in os.listdir('/content/musical-instrument-chord-classification/'):
    for j in os.listdir('/content/musical-instrument-chord-classification/'+i):
        for k in os.listdir('/content/musical-instrument-chord-classification/'+i+'/'+j):
            audio_files.append('/content/musical-instrument-chord-classification/'+i+'/'+j+'/'+k)
            audio_labels.append(j)

df = pd.DataFrame({'file': audio_files, 'label': audio_labels})
df.head()

plt.figure(figsize=(5,5))
plt.bar(df['label'].value_counts().index, df['label'].value_counts().values)
plt.show()

# min_size = df['label'].value_counts().min()
# df = (
#     df.groupby('label', group_keys=False)
#       .apply(lambda x: x.sample(n=min_size, random_state=42))
# )

# print(df['label'].value_counts())

train_df = df.sample(frac=0.9, random_state=42)
test_df = df.drop(train_df.index)

val_df = test_df.sample(frac=0.5, random_state=42)
test_df = test_df.drop(val_df.index)

print(f'Train size: {len(train_df)}')
print(f'Val size: {len(val_df)}')
print(f'Test size: {len(test_df)}')

label_encoder = LabelEncoder()

class CustomAudioDataset(Dataset):

  def __init__(self, df):
    self.df = df
    self.labels = torch.tensor(label_encoder.fit_transform(self.df['label']), dtype=torch.int64).to(device)
    self.audios = [torch.tensor(self.preprocess(path) , dtype=torch.float32).to(device) for path in self.df['file']]

  def __len__(self):
    return len(self.df)

  def __getitem__(self, index):
     return self.audios[index].to(device), self.labels[index].to(device)

  def preprocess(self, path):
      duration = 2
      h, w = 32, 64

      audio, sr = librosa.load(path, sr=22050)
      spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=2048, hop_length=512, n_mels=h)
      spec = librosa.power_to_db(spec, ref=np.max)
      spec = librosa.util.fix_length(spec, size=(duration*sr)//512+1)
      spec = resize(spec, (h, w), anti_aliasing=True)

      return spec

train_ds = CustomAudioDataset(train_df)
val_ds = CustomAudioDataset(val_df)
test_ds = CustomAudioDataset(test_df)

LR = 1e-3
BATCH_SIZE = 8
EPOCHS = 25

train = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)
test = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)

class Net(nn.Module):
    def __init__(self):
        super().__init__()

        self.features = nn.Sequential(
            # Block 1: low-level features
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2),   # 16 × 16 × 32

            # Block 2: mid-level features
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),   # 32 × 8 × 16

            # Spatial aggregation
            nn.AdaptiveAvgPool2d((1, 1))  # 64 × 1 × 1
        )

        # Minimal classifier head
        self.classifier = nn.Sequential(
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 2)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)  # 64 x 1
        x = self.classifier(x)
        return x

net = Net()
net.to(device)

optim = Adam(net.parameters(), lr=LR)
loss_fn = nn.CrossEntropyLoss()

train_loss_plot = []
val_loss_plot = []
train_acc_plot = []
val_acc_plot = []

for epoch in range(EPOCHS):

    # =========================
    # TRAIN
    # =========================
    net.train()

    train_loss = 0.0
    train_correct = 0
    train_samples = 0

    for inputs, labels in train:
        inputs = inputs.to(device).float()/255.0
        labels = labels.to(device) # Move labels to the same device as model

        optim.zero_grad()

        # Add a channel dimension: (batch_size, H, W) -> (batch_size, 1, H, W)
        preds = net(inputs.unsqueeze(1))
        loss = loss_fn(preds, labels)

        loss.backward()
        optim.step()

        train_loss += loss.item()

        _, predicted = torch.max(preds.data, 1) # Correct way to get class predictions
        train_correct += (predicted == labels).sum().item()
        train_samples += labels.size(0)

    train_loss /= len(train)
    train_acc = (train_correct / train_samples) * 100

    train_loss_plot.append(train_loss)
    train_acc_plot.append(train_acc)

    # =========================
    # VALIDATION
    # =========================
    net.eval()

    val_loss = 0.0
    val_correct = 0
    val_samples = 0

    with torch.no_grad():
        for inputs, labels in val:
            inputs = inputs.to(device).float()/255.0
            labels = labels.to(device) # Move labels to the same device as model

            # Add a channel dimension: (batch_size, H, W) -> (batch_size, 1, H, W)
            preds = net(inputs.unsqueeze(1))
            loss = loss_fn(preds, labels)

            val_loss += loss.item()
            _, predicted = torch.max(preds.data, 1) # Correct way to get class predictions
            val_correct += (predicted == labels).sum().item()
            val_samples += labels.size(0)

    val_loss /= len(val)
    val_acc = (val_correct / val_samples) * 100

    val_loss_plot.append(val_loss)
    val_acc_plot.append(val_acc)

    # =========================
    # LOG
    # =========================
    if epoch % 1 == 0:
        print(
            f'Epoch {epoch+1:03d} | '
            f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | '
            f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%'
        )

net.eval()

test_loss = 0.0
test_correct = 0
test_samples = 0

with torch.no_grad():
    for inputs, labels in test:
        inputs = inputs.to(device).float()/255.0
        labels = labels.to(device)

        preds = net(inputs.unsqueeze(1))
        loss = loss_fn(preds, labels)

        test_loss += loss.item()
        _, predicted = torch.max(preds.data, 1)
        test_correct += (predicted == labels).sum().item()
        test_samples += labels.size(0)

test_loss /= len(test)
test_accuracy = (test_correct / test_samples) * 100

print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%')

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_loss_plot, label='Train Loss')
plt.plot(val_loss_plot, label='Validation Loss')
plt.title('Loss over Epochs')
plt.xlim(0, EPOCHS)
plt.ylim(0, 1)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_acc_plot, label='Train Accuracy')
plt.plot(val_acc_plot, label='Validation Accuracy')
plt.title('Accuracy over Epochs')
plt.xlim(0, EPOCHS)
plt.ylim(0, 100)
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()

plt.tight_layout()
plt.show()

