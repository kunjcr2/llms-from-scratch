# -*- coding: utf-8 -*-
"""Bean Lead Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vNPiFa19vlcDm3L2rRBu-kFQ6sNPZgbQ
"""

# !pip install -q opendatasets
import opendatasets as od
od.download("https://www.kaggle.com/datasets/marquis03/bean-leaf-lesions-classification")

import torch
import torch.nn as nn
from torch.optim import Adam
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, Dataset
from torchvision import models

from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from PIL import Image
import pandas as pd
import numpy as np
import os

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

train_df = pd.read_csv("/content/bean-leaf-lesions-classification/train.csv")
val_df = pd.read_csv("/content/bean-leaf-lesions-classification/val.csv")

train_df["image:FILE"] = "/content/bean-leaf-lesions-classification/" + train_df["image:FILE"]
val_df["image:FILE"] = "/content/bean-leaf-lesions-classification/" + val_df["image:FILE"]

print("Training Shape:" + str(train_df.shape))
print("Val Shape: " + str(val_df.shape))
print(train_df["category"].value_counts())

transform = transforms.Compose([
    transforms.Resize((128,128)),
    transforms.ToTensor(),
    transforms.ConvertImageDtype(torch.float32)
])

class CustomImageDataset(Dataset):
  def __init__(self, df, transform):
    self.df = df
    self.transform = transform
    self.labels = torch.tensor(df["category"]).to(device)

  def __len__(self):
    return self.df.shape[0]

  def __getitem__(self, idx):
    img_path = self.df.iloc[idx, 0]
    label = self.labels[idx]

    image = Image.open(img_path).convert('RGB')

    if self.transform:
      image = (self.transform(image)/255.0).to(device)

    return image, label

train_dataset = CustomImageDataset(train_df, transform)
val_dataset = CustomImageDataset(val_df, transform)

n_rows = 3
n_cols = 3
f, axarr = plt.subplots(n_rows, n_cols)

for row in range(n_rows):
  for col in range(n_cols):
    img = train_dataset[np.random.randint(0, len(train_dataset))][0].cpu()
    axarr[row, col].imshow((img*255.0).squeeze().permute(1,2,0))
    axarr[row, col].axis('off')

plt.tight_layout()
plt.show()

LR = 1e-3
BATCH_SIZE = 4
EPOCHS = 10

train = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True
)

val = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True
)

model = models.googlenet(weights="DEFAULT").to(device)
for param in model.parameters():
  param.requires_grad = True

num_class = len(train_df["category"].unique())
model.fc = nn.Linear(model.fc.in_features, num_class).to(device)

## FINETUNING - WHOLE MODEL - ~80%-84% val accuracy

loss_fn = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=LR)

loss_train_plot, acc_train_plot = [], []

for epoch in range(EPOCHS):
  train_loss, train_acc = 0.0, 0.0

  for inputs, labels in train:
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    train_loss += loss.item()
    train_acc += (outputs.argmax(dim=1) == labels).sum().item()

  loss_train_plot.append(train_loss/len(train))
  acc_train_plot.append(train_acc/len(train_dataset))

  print(f"TRAIN Epoch: {epoch+1}/{EPOCHS} | Loss: {round(train_loss/len(train), 4)} | Acc: {round(train_acc/len(train_dataset)*100, 2)}%")

  with torch.no_grad():
    val_loss, val_acc = 0.0, 0.0
    for inputs, labels in val:
      outputs = model(inputs)
      loss = loss_fn(outputs, labels)

      val_loss += loss.item()
      val_acc += (outputs.argmax(dim=1) == labels).sum().item()

  print(f"VAL Epoch: {epoch+1}/{EPOCHS} | Val Loss: {round(val_loss/len(val), 4)} | Val Acc: {round(val_acc/len(val_dataset)*100, 2)}%")

# We didnt make a test dataset so, NO Inference, but same as before !

## Transfer Learning - ON LAST LAYER - ~55%-60%

model_2 = models.googlenet(weights="DEFAULT")
for param in model_2.parameters():
    param.requires_grad = False

model_2.fc = torch.nn.Linear(
    model.fc.in_features, num_class
)

model_2.fc.requires_grad = True
model_2 = model_2.to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = Adam(model_2.parameters(), lr=LR)

loss_train_plot, acc_train_plot = [], []

for epoch in range(EPOCHS):
  train_loss, train_acc = 0.0, 0.0

  for inputs, labels in train:
    optimizer.zero_grad()
    outputs = model_2(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    train_loss += loss.item()
    train_acc += (outputs.argmax(dim=1) == labels).sum().item()

  loss_train_plot.append(train_loss/len(train))
  acc_train_plot.append(train_acc/len(train_dataset))

  print(f"TRAIN Epoch: {epoch+1}/{EPOCHS} | Loss: {round(train_loss/len(train), 4)} | Acc: {round(train_acc/len(train_dataset)*100, 2)}%")

  with torch.no_grad():
    val_loss, val_acc = 0.0, 0.0
    for inputs, labels in val:
      outputs = model_2(inputs)
      loss = loss_fn(outputs, labels)

      val_loss += loss.item()
      val_acc += (outputs.argmax(dim=1) == labels).sum().item()

  print(f"VAL Epoch: {epoch+1}/{EPOCHS} | Val Loss: {round(val_loss/len(val), 4)} | Val Acc: {round(val_acc/len(val_dataset)*100, 2)}%")

